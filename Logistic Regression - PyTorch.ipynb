{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes. Given data on **time spent studying** and **exam scores**. Linear Regression and logistic regression can predict different things:\n",
    "\n",
    "- **Linear Regression** could help us predict the student’s test score on a scale of 0 - 100. Linear regression predictions are continuous (numbers in a range).\n",
    "- **Logistic Regression** could help use predict whether the student passed or failed. Logistic regression predictions are discrete (only specific values or categories are allowed). We can also view probability scores underlying the model’s classifications.\n",
    "\n",
    "### Types of Logistic Regression\n",
    "\n",
    "- Binary (Pass/Fail)\n",
    "- Multi (Cats, Dogs, Sheep)\n",
    "- Ordinal (Low, Medium, High)\n",
    "\n",
    "In this notebook, we are going to classify handwritten digits from the MNIST dataset using Logistic Regression in PyTorch. This is a multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset\n",
    "\n",
    "Before we define our model, let's go ahead and import our dataset. The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems. The database has a training set of 60,000 examples, and a test set of 10,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 784  # Our images are 28px by 28px in size\n",
    "num_classes = 10  # We have handwritten digits from 0 - 9\n",
    "num_epochs = 5  # Number of epochs\n",
    "batch_size = 100  # Batch size\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "transfm = transforms.ToTensor()  # Transform the dataset objects to tensors\n",
    "\n",
    "# MNIST dataset - images and labels\n",
    "train_dataset = datasets.MNIST(root='./data',\n",
    "                               train=True,\n",
    "                               transform=transfm,\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data',\n",
    "                              train=False,\n",
    "                              transform=transfm)\n",
    "\n",
    "# Input pipeline\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `torch.nn` module contains the code required for the model, `torchvision.datasets` contains the MNIST dataset. It contains the dataset of handwritten digits that we shall be using here. The `torchvision.transforms` module contains various methods to transform objects into others. Here, we shall be using it to transform from images to PyTorch tensors. Also, the `torch.autograd` module contains the `Variable` class amongst others, which will be used by us while defining our tensors.\n",
    "\n",
    "In our dataset, the image size is 28x28. Thus, our input size is 784. Also, 10 digits are present in this dataset and hence, we can have 10 different outputs. Thus, we set `num_classes` as 10. Also, we shall train our model for five times on the entire dataset. Finally, we will train in small batches of 100 images each so as to prevent the crashing of the program due to memory overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "Here, we shall initialise our model as a subclass of `torch.nn.Module` and then define the forward pass. In the code that we are writing, the softmax is internally calculated during each forward pass and hence we do not need to specify it inside the `forward()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_hat = self.linear(x)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "\n",
    "Next, we set our loss function and the optimiser. Here, we shall be using the cross entropy loss and for the optimiser, we shall be using the stochastic gradient descent algorithm with a learning rate of 0.001 as defined in the hyper parameter above. \n",
    "\n",
    "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
    "\n",
    "![Imgur](https://i.imgur.com/uLXWRg8.png)\n",
    "\n",
    "\n",
    "The graph above shows the range of possible loss values given a true observation. As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications that are confident and wrong!\n",
    "\n",
    "Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now, we shall start the training. Here, we shall be performing the following tasks:\n",
    "\n",
    "1. Reset all gradients to 0.\n",
    "2. Make a forward pass.\n",
    "3. Calculate the loss.\n",
    "4. Perform backpropagation.\n",
    "5. Update all weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss :2.2049152851104736\n",
      "Epoch 1, loss :2.0981056690216064\n",
      "Epoch 1, loss :2.0286219120025635\n",
      "Epoch 1, loss :1.917850375175476\n",
      "Epoch 1, loss :1.8506721258163452\n",
      "Epoch 1, loss :1.810417652130127\n",
      "Epoch 2, loss :1.7023305892944336\n",
      "Epoch 2, loss :1.680514931678772\n",
      "Epoch 2, loss :1.6131799221038818\n",
      "Epoch 2, loss :1.5625419616699219\n",
      "Epoch 2, loss :1.525355577468872\n",
      "Epoch 2, loss :1.453291416168213\n",
      "Epoch 3, loss :1.4182931184768677\n",
      "Epoch 3, loss :1.364874005317688\n",
      "Epoch 3, loss :1.3616888523101807\n",
      "Epoch 3, loss :1.3887336254119873\n",
      "Epoch 3, loss :1.2908176183700562\n",
      "Epoch 3, loss :1.3103325366973877\n",
      "Epoch 4, loss :1.2904797792434692\n",
      "Epoch 4, loss :1.2262612581253052\n",
      "Epoch 4, loss :1.1611058712005615\n",
      "Epoch 4, loss :1.1176444292068481\n",
      "Epoch 4, loss :1.0963448286056519\n",
      "Epoch 4, loss :1.0735220909118652\n",
      "Epoch 5, loss :1.1075222492218018\n",
      "Epoch 5, loss :1.0989426374435425\n",
      "Epoch 5, loss :1.0390461683273315\n",
      "Epoch 5, loss :0.968352735042572\n",
      "Epoch 5, loss :1.0160298347473145\n",
      "Epoch 5, loss :1.105699062347412\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28 * 28))  # Images flattened into 1D tensors\n",
    "        labels = Variable(labels)  # Labels \n",
    "        \n",
    "        # Forward -> Backprop -> Optimize\n",
    "        optimizer.zero_grad()  # Manually zero the gradient buffers\n",
    "        outputs = model(images)  # Predict the class using the test set\n",
    "        loss = criterion(outputs, labels)  # Compute the loss given the predicted label\n",
    "                                           # and actual label\n",
    "        \n",
    "        loss.backward()  # Compute the error gradients\n",
    "        optimizer.step()  # Optimize the model via Stochastic Gradient Descent\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Epoch {}, loss :{}\".format(epoch + 1, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now, let's test our model and see how accurate our model can classify handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.82%\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28 * 28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    " \n",
    "print('Accuracy: {}%'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
